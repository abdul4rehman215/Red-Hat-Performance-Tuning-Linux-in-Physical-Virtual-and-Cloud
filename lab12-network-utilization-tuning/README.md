# ğŸ§ª Lab 12: Network Utilization Tuning

> **Focus:** Improving Linux network throughput and stability by tuning **TCP buffers (sysctl)**, optimizing **NIC settings (ethtool)**, validating performance with **iperf3**, and making optimizations **persistent** using **systemd**.

---

## ğŸ“Œ Lab Summary

In this lab, I performed a complete **network performance tuning workflow** on an Ubuntu cloud lab setup using two VMs (server + client). I started by collecting baseline network configuration and throughput measurements, then applied performance optimizations at two layers:

1. **Kernel networking (sysctl):** Increased TCP buffer limits, enabled modern congestion control (**BBR**), and tuned backlog settings.
2. **NIC tuning (ethtool):** Increased ring buffers, enabled beneficial offloads, adjusted interrupt coalescing, and created a **systemd oneshot service** to persist settings.

Finally, I executed advanced iperf3 tests and monitored system resources during load using **sar (sysstat)**, producing comparison outputs saved to a results directory for reporting and GitHub documentation.

---

## ğŸ¯ Objectives

By the end of this lab, I was able to:

- Understand fundamentals of network performance optimization in Linux
- Modify TCP buffer sizes using **sysctl** to improve throughput
- Configure network interface performance features using **ethtool**
- Measure and compare network performance using **iperf3**
- Apply a repeatable tuning methodology: **baseline â†’ optimize â†’ validate â†’ persist**
- Troubleshoot typical performance and tooling issues in cloud Linux environments

---

## âœ… Prerequisites

- Linux command-line basics
- Understanding of TCP/IP (bandwidth, latency, RTT, buffers)
- Sudo/root privileges
- Familiarity with performance tooling (`iperf3`, `ethtool`, `sysctl`, `sar`)

---

## ğŸ§° Lab Environment

- **Platform:** Cloud lab VMs (2-node topology)
- **OS Used:** Ubuntu cloud lab
- **Nodes:**
  - **Server VM:** `ip-172-31-10-201` (NIC: `ens5`, IP: `172.31.20.10`)
  - **Client VM:** `ip-172-31-10-202` (NIC: `ens5`, IP: `172.31.20.11`)
- **Tools:**
  - `iperf3`, `ethtool`, `sysctl`, `sysstat (sar)`, `ping`
  - Additional utilities installed during lab: `net-tools` (for `ifconfig`), `bc`

> âš ï¸ Note: This lab includes commands run on **both** server and client. Output artifacts are organized accordingly.

---

## ğŸ—‚ï¸ Repository Structure

```text
lab12-network-utilization-tuning/
â”œâ”€â”€ README.md
â”œâ”€â”€ commands.sh
â”œâ”€â”€ output.txt
â”œâ”€â”€ interview_qna.md
â”œâ”€â”€ troubleshooting.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ check_tcp_buffers.sh
â”‚   â”œâ”€â”€ calculate_buffers.sh
â”‚   â”œâ”€â”€ analyze_interface.sh
â”‚   â”œâ”€â”€ optimize_ring_buffers.sh
â”‚   â”œâ”€â”€ optimize_offloads.sh
â”‚   â”œâ”€â”€ optimize_coalescing.sh
â”‚   â”œâ”€â”€ ethtool_persistent.sh
â”‚   â”œâ”€â”€ comprehensive_test.sh
â”‚   â”œâ”€â”€ monitor_performance.sh
â”‚   â”œâ”€â”€ latency_analysis.sh
â”‚   â””â”€â”€ performance_comparison.sh
â””â”€â”€ configs/
    â”œâ”€â”€ 99-network-performance.conf
    â””â”€â”€ network-optimization.service
````

---

## âœ… Tasks Overview

### **Task 1: Baseline Network Performance Assessment**

* Identified NIC and IP configuration using `ip addr`, `ip -s link`
* Collected baseline TCP buffer and network sysctl values
* Validated NIC capabilities and defaults using `ethtool` (speed, ring buffers, offloads)
* Established baseline throughput using:

  * `iperf3` TCP (30s)
  * Saved baseline TCP/UDP/latency results into `~/network_tuning_results/baseline_results.txt`

### **Task 2: TCP Buffer Size Optimization**

* Audited existing TCP buffer configuration using a script (`check_tcp_buffers.sh`)
* Estimated optimal buffer size using a bandwidth-delay product approach (`calculate_buffers.sh`)
* Applied tuned sysctl parameters via:

  * `/etc/sysctl.d/99-network-performance.conf`
  * Increased max/default buffers, backlog values
  * Enabled **BBR** congestion control and TCP Fast Open
* Verified settings after applying sysctl changes
* Re-tested throughput and saved results to `tcp_optimized_results.txt`

### **Task 3: NIC Optimization with ethtool**

* Generated full interface analysis report (ring buffers, offloads, coalescing, driver info)
* Increased RX/TX ring buffers to maximum supported values
* Confirmed and enabled key offloads (where supported): TSO/GSO/GRO + checksum + scatter-gather
* Tuned interrupt coalescing to balance throughput and CPU usage
* Made ethtool settings persistent using:

  * `/usr/local/bin/ethtool_persistent.sh`
  * systemd oneshot service: `network-optimization.service`

### **Task 4: Comprehensive Performance Testing & Analysis**

* Ran multi-scenario performance tests (TCP window sizes, UDP rates, parallel streams, bidirectional/reverse)
* Captured results to `comprehensive_results.txt`
* Monitored CPU/memory/network stats during tests using `sar`
* Produced a comparison report extracting key throughput results and summarizing applied tuning:

  * `performance_comparison.txt`

---

## ğŸ§ª Validation & Evidence

Validation steps performed:

* Confirmed active interface + link stats (`ip -s link`, `ethtool`)
* Verified sysctl tuning applied (`sysctl -p` + re-run check script)
* Verified congestion control (`tcp_available_congestion_control`, `tcp_congestion_control`)
* Verified ring buffer changes (`ethtool -g`)
* Verified persistent service is enabled and active:

  * `systemctl status network-optimization.service`
* Confirmed results artifacts exist and contain data:

  * `baseline_results.txt`, `tcp_optimized_results.txt`, `comprehensive_results.txt`
  * `system_monitoring.txt`, `latency_analysis.txt`, `performance_comparison.txt`

---

## ğŸ“ˆ Observations (High-Level)

* Baseline throughput was measured first to avoid â€œtuning without proofâ€
* TCP buffer increases + BBR produced improved throughput under parallel load scenarios
* Ring buffer and coalescing tuning improved stability under bursty traffic patterns
* Some NIC features may show as `[fixed]` or â€œnot supportedâ€ in virtual/cloud NICsâ€”handled gracefully

---

## ğŸ§  What I Learned

* How TCP buffer sizing impacts throughput, especially on high-speed NICs
* How to safely apply network sysctl changes via `/etc/sysctl.d/`
* Practical ethtool tuning: ring buffers, offloads, interrupt coalescing
* How to make interface tuning persistent using systemd
* How to measure results properly using iperf3 and system monitoring with sar

---

## ğŸŒ Why This Matters

Network tuning is critical for:

* high-throughput systems (log pipelines, replication, backups)
* latency-sensitive services (APIs, trading systems, microservices)
* cloud environments with 10Gb+ NICs where defaults can limit performance
* production reliability (reducing retransmits, drops, and queue pressure)

---

## âœ… Conclusion

This lab demonstrated a full, repeatable network performance tuning workflow:

âœ… Baseline measured with iperf3
âœ… TCP buffers tuned and applied persistently via sysctl.d
âœ… NIC optimized using ethtool (ring buffers, offloads, coalescing)
âœ… Persistence implemented using a systemd oneshot service
âœ… Advanced testing + monitoring performed with results stored for reporting

All commands, scripts, outputs, and configuration artifacts are included in this lab folder for GitHub portfolio documentation.

---

