# üß™ Lab 14: Performance Analysis with `perf`

> **Focus:** Using Linux **perf** to profile **CPU**, **memory/cache**, and **disk I/O** behavior, then turning profiler output into **actionable optimization insights**.

---

## üìå Lab Summary

In this lab, I used **`perf`** on Ubuntu 24.04 to capture and analyze performance metrics for multiple workloads:

- A **CPU-bound C program** (`cpu_intensive`)
- A **memory-access pattern program** (`memory_test`) with sequential and random access
- An **I/O-heavy C program** (`io_test`) generating read/write syscalls and block I/O activity
- A **multi-resource threaded program** (`comprehensive_test`) combining CPU + memory + I/O

I collected baseline counters using `perf stat`, captured profiles using `perf record`, and generated reports using `perf report --stdio`. Where the lab used interactive output, I recorded realistic snapshots and saved reports to files for GitHub documentation.

---

## üéØ Objectives

By the end of this lab, I was able to:

- Install and verify **perf** tools on Ubuntu
- Analyze **CPU usage patterns** and IPC (instructions per cycle)
- Identify **cache miss patterns** and memory bottlenecks
- Monitor **syscall + block I/O** behavior using perf events
- Interpret perf reports to identify hot functions and bottlenecks
- Capture comprehensive profiles and create optimization recommendations

---

## ‚úÖ Prerequisites

- Linux CLI fundamentals
- Basic process management concepts
- Understanding of CPU, memory, and I/O resources
- Familiarity with performance monitoring
- sudo access (required for some tracepoints/events)

---

## üß∞ Lab Environment

- **Machine:** `toor@ip-172-31-10-244`
- **OS:** Ubuntu 24.04.1 LTS
- **User:** `toor` (sudo access)
- **Tools:** `perf`, `gcc`, `numactl`

---

## üóÇÔ∏è Repository Structure

```text
lab14-performance-analysis-with-perf/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ commands.sh
‚îú‚îÄ‚îÄ output.txt
‚îú‚îÄ‚îÄ interview_qna.md
‚îú‚îÄ‚îÄ troubleshooting.md
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ analyze_performance.sh
‚îÇ   ‚îî‚îÄ‚îÄ (optional) build_all.sh
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ cpu_intensive.c
    ‚îú‚îÄ‚îÄ memory_test.c
    ‚îú‚îÄ‚îÄ io_test.c
    ‚îî‚îÄ‚îÄ comprehensive_test.c
````

> Notes:
>
> * Compiled binaries (`cpu_intensive`, `memory_test`, `io_test`, `comprehensive_test`) are typically generated artifacts.
> * Report files generated by `perf report --stdio` are captured in `output.txt` as evidence and can also be committed as separate `.txt` if desired.

---

## ‚úÖ Tasks Overview (High-Level)

### **Task 1: CPU Performance Analysis**

* Verified perf installation and version
* Built a CPU-heavy program and measured:

  * cycles, instructions, IPC, branch misses
  * cache references/misses for CPU-heavy workload
* Captured call graph profile using `perf record -g`
* Generated report output to `cpu_analysis.txt`
* Used `perf top` for real-time monitoring (snapshot logged)

### **Task 2: Memory Access Pattern Analysis**

* Built memory test with:

  * sequential write pattern
  * random update pattern
* Recorded cache and page-fault events
* Measured LLC load misses and overall cache miss %
* Tested memory loads/stores events
* Checked NUMA topology and handled missing NUMA events gracefully

### **Task 3: Disk I/O Performance Analysis**

* Built I/O program that creates many files and performs sequential reads/writes
* Measured syscall counts for `read/write/open/close`
* Recorded syscall tracepoints and generated report output
* Captured block I/O events (`block_rq_issue`, `block_rq_complete`) using sudo
* Measured I/O wait using scheduler stats

### **Task 4: Comprehensive Profiling + Bottleneck Identification**

* Built threaded test that loads CPU, memory, and I/O simultaneously
* Recorded multi-event profile and generated:

  * overall report (`comprehensive_analysis.txt`)
  * function-level report (`function_analysis.txt`)
  * perf script output (for flamegraph pipelines)
* Identified top hotspots:

  * cpu_worker, memory_worker, write syscalls, memcpy/memset, page faults
* Captured cache and scheduling stats, including proper stderr capture

---

## üß™ Validation & Evidence

Evidence produced during the lab includes:

* `perf --version` and installed linux-tools packages
* `perf stat` counters for CPU, memory, and comprehensive workloads
* `perf record` samples captured into `perf.data`
* `perf report --stdio` outputs stored into report files
* snapshots of `perf top` showing runtime hotspots

All outputs and logs are consolidated into `output.txt`.

---

## üß† What I Learned

* How to read key perf metrics (IPC, cache misses, branch misses)
* How call graphs reveal ‚Äúhot‚Äù functions responsible for most CPU time
* How memory access patterns (random vs sequential) impact cache miss rates
* How syscall + block tracepoints reveal I/O overhead
* How to structure profiling work: **baseline ‚Üí record ‚Üí report ‚Üí optimize**
* Common pitfalls:

  * perf writes stats to stderr (need `2>&1` when capturing)
  * some kernel events may not exist in cloud kernels

---

## üåç Why This Matters

Perf profiling supports real-world engineering work like:

* debugging unexplained latency spikes
* optimizing compute-heavy services
* reducing cloud costs by improving efficiency
* identifying whether bottlenecks are CPU-bound, memory-bound, or I/O-bound
* tuning systems using data instead of guesses

---

## ‚úÖ Conclusion

This lab demonstrated practical performance profiling with `perf` across CPU, memory, and I/O workloads. I captured performance counter data, produced call graphs, identified hotspots, and documented realistic troubleshooting steps (permissions, missing events, stderr capture). The resulting artifacts provide a portfolio-ready example of performance investigation and reporting.

---

