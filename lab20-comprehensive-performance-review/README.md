# ğŸ§ª Lab 20: Comprehensive Performance Review

## ğŸ“Œ Overview
This lab is a **full-system performance review** where we monitored **CPU, memory, disk I/O, and network** *together* (not one tool at a time), identified bottlenecks from real data, applied safe tuning, and validated changes with before/after testing.

---

## ğŸ¯ Objectives
- âœ… Perform holistic performance analysis using multiple tools simultaneously
- âœ… Identify bottlenecks across **CPU, Memory, Disk I/O, Network**
- âœ… Apply tuning changes based on measured evidence
- âœ… Document changes and validate optimization effectiveness
- âœ… Build a repeatable workflow for production performance reviews

---

## âœ… Prerequisites
- Linux CLI basics (navigation, permissions, editors)
- Understanding of CPU / Memory / Disk / Network metrics
- Familiarity with monitoring concepts from previous labs
- Sudo/root access for tuning changes
- Basic shell scripting knowledge

---

## ğŸ–¥ï¸ Lab Environment
**Ubuntu Cloud Lab**
- **Machine:** `toor@ip-172-31-10-267`
- **OS:** Ubuntu 24.04.1 LTS
- **CPU:** 2 vCPU (burstable cloud profile)
- **RAM:** 4GB
- **NIC:** `ens5`

---

## ğŸ§© Tools Used
- **CPU / Process:** `top`, `htop`
- **Disk I/O:** `iostat`
- **System Activity:** `sar`
- **System Snapshot:** `free`, `df`, `uptime`
- **Scheduling / Memory pressure:** `vmstat`
- **Benchmarking:** `dd`, `iperf3`
- **Scripting + reports:** Bash scripts + timestamped logs

---

## ğŸ§ª Task Summary (High-Level)
### ğŸŸ¦ Task 1: Baseline + Concurrent Monitoring
- Created a workspace under `/opt/performance-review`
- Captured **system baseline info**
- Ran a **5-minute monitoring window** collecting:
  - `top`, `iostat`, `sar`, `vmstat`, periodic `free`
- Stored outputs in **timestamped folders** for repeatability

### ğŸŸ© Task 2: Data Analysis + Bottleneck Identification
- Parsed collected logs to compute:
  - Avg CPU usage + top CPU processes
  - Avg disk utilization + high-util devices
  - Avg memory usage
  - Avg network RX/TX
- Verified tunables (swappiness, dirty ratios, scheduler, TCP settings)

### ğŸŸ¨ Task 3: Apply Data-Driven Tuning
Applied tuning safely (with backups + persistent config):
- Memory tuning:
  - `vm.swappiness = 10`
  - `vm.dirty_ratio = 15`
  - `vm.dirty_background_ratio = 5`
- Network buffers:
  - `net.core.rmem_max = 16777216`
  - `net.core.wmem_max = 16777216`
- TCP:
  - `net.ipv4.tcp_window_scaling = 1`
- Created persistence file:
  - `/etc/sysctl.d/99-performance-tuning.conf`

### ğŸŸ¥ Task 4: Post-Tuning Validation + Documentation
- Ran post-tuning tests for:
  - CPU task timing
  - Memory write speed (`dd`)
  - Disk write/read throughput (`dd`)
  - Network loopback (`iperf3`)
- Generated **comparison report** (baseline vs post-tuning)
- Documented the tuning + validation notes

---

## ğŸ“Š Results Summary
- Monitoring produced actionable evidence:
  - CPU spikes during stress window
  - High disk utilization on `nvme0n1` during I/O stress
  - Memory usage average around ~60% during workload
- After tuning:
  - Lower swappiness reduces unnecessary swap tendency
  - Lower dirty ratios improve writeback behavior (more responsive under I/O pressure)
  - Increased network buffers improve throughput stability
  - Persistent config ensures settings survive reboot

---

## ğŸ§  What I Learned
- âœ… One tool alone can lie â€” **correlation across tools** reveals the real bottleneck.
- âœ… Collecting data **during real load** is mandatory for meaningful tuning.
- âœ… Always apply tuning with:
  - backups
  - verification
  - persistence strategy
- âœ… â€œBefore/Afterâ€ must be **apples-to-apples** (same stress test window) for production.

---

## ğŸŒ Why This Matters
In production, performance issues rarely belong to one subsystem.
This workflow helps teams:
- prevent random tuning guesses
- find real root causes
- validate improvements safely
- maintain documentation and rollback plans

---

## ğŸ¢ Real-World Relevance
- SRE/DevOps incident response (slow apps, spikes, saturation)
- Capacity planning and baseline building
- Cloud cost optimization (right-sizing based on metrics)
- Post-change validation after system upgrades/tuning
- Auditable performance reviews for production servers

---

## ğŸ“ Repo Structure
```bash
lab20-comprehensive-performance-review/
â”œâ”€â”€ README.md
â”œâ”€â”€ commands.sh
â”œâ”€â”€ output.txt
â”œâ”€â”€ interview_qna.md
â”œâ”€â”€ troubleshooting.md
â”œâ”€â”€ baseline/
â”‚   â””â”€â”€ system_info.txt
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ <timestamp>/
â”‚       â”œâ”€â”€ top_output.txt
â”‚       â”œâ”€â”€ iostat_output.txt
â”‚       â”œâ”€â”€ sar_output.txt
â”‚       â”œâ”€â”€ vmstat_output.txt
â”‚       â””â”€â”€ memory_tracking.txt
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ performance_analysis_<timestamp>.txt
â”‚   â””â”€â”€ performance_comparison_<timestamp>.txt
â””â”€â”€ scripts/
    â”œâ”€â”€ performance_monitor.sh
    â”œâ”€â”€ cpu_stress.sh
    â”œâ”€â”€ io_stress.sh
    â”œâ”€â”€ analyze_performance.sh
    â”œâ”€â”€ identify_bottlenecks.sh
    â”œâ”€â”€ tuning_recommendations.sh
    â”œâ”€â”€ apply_tuning.sh
    â”œâ”€â”€ verify_tuning.sh
    â”œâ”€â”€ post_tuning_test.sh
    â””â”€â”€ compare_performance.sh
````

---

## âœ… Conclusion

This lab completed a full **end-to-end performance review cycle**:
**baseline â†’ monitor â†’ analyze â†’ tune â†’ verify â†’ validate â†’ document**.

By combining multiple monitoring tools, we avoided guesswork and applied tuning changes backed by measurable system behaviorâ€”exactly how performance reviews should be done in real production environments.

